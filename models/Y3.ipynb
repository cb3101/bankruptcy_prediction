{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install scikit-learn==0.20.1  # dep for missingpy\n",
    "#!pip3 install scikit-learn==0.24.1  # required for getting tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year $N$\n",
    "Run all in this section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = '3'\n",
    "drop_cols = ['Attr37', 'Attr7','Attr43','Attr62','Attr32','Attr44','Attr15','Attr19','Attr3','Attr51','Attr4','Attr49','Attr38','Attr60','Attr6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff = loadarff(data_path+N+'year.arff')\n",
    "df = pd.DataFrame(arff[0])\n",
    "df['class']= df['class'].astype('int')\n",
    "df = df.drop_duplicates()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attr37    4703\n",
       "Attr21     805\n",
       "Attr27     713\n",
       "Attr60     589\n",
       "Attr45     588\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change dropcols accordingly...\n",
    "df.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(drop_cols,axis='columns')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain NaN in test set also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7442\n",
       "1     370\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    2481\n",
       "1     123\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']\n",
    "# (X.shape,Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=random_state,shuffle=True,stratify=Y)\n",
    "\n",
    "display(Y_train.value_counts(), Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test,Y_test = X_train.to_numpy(),Y_train.to_numpy(),X_test.to_numpy(),Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "#scaler = None\n",
    "\n",
    "#idx = np.isnan(X_train).any(axis=1)\n",
    "#scaler = StandardScaler().fit(X_train[~idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_imputer(imputer_estimator,X_train,Y_train,transform_x=True,reset_index = True,verbose=True,max_iter=10,tol=1,imputer=None,scaler=None):\n",
    "    # train = pd.concat([X_train,Y_train],axis=1)\n",
    "    # train['class'] = train['class'].astype('category')\n",
    "    # if reset_index:\n",
    "    #     train = train.reset_index(drop=True)\n",
    "    \n",
    "    if imputer is None:\n",
    "        imputer = IterativeImputer(estimator=imputer_estimator, n_nearest_features=None, imputation_order='descending',verbose=verbose,max_iter=max_iter,tol=tol)\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "    else:\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "        \n",
    "    \n",
    "    if transform_x:\n",
    "        X_train = imputer.transform(X_train)\n",
    "        \n",
    "        #if scaler is not None:\n",
    "        #    X_train = scaler.transform(X_train)\n",
    "        \n",
    "        return imputer,X_train\n",
    "    \n",
    "    return imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom missingpy import MissForest\\n\\nmissf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\\nmissf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\\nX_test_imp = missf_imp.transform(X_test)\\n\\n# note... these are not scaled!!\\nnp.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\\nnp.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from missingpy import MissForest\n",
    "\n",
    "missf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\n",
    "missf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\n",
    "X_test_imp = missf_imp.transform(X_test)\n",
    "\n",
    "# note... these are not scaled!!\n",
    "np.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\n",
    "np.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7812, 49)\n",
      "[IterativeImputer] Change: 43.61759626233691, scaled tolerance: 30.88706761792681 \n",
      "[IterativeImputer] Change: 40.761957147612534, scaled tolerance: 30.88706761792681 \n",
      "[IterativeImputer] Change: 29.99714999735972, scaled tolerance: 30.88706761792681 \n",
      "[IterativeImputer] Early stopping criterion reached.\n"
     ]
    }
   ],
   "source": [
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# has to be scaled...\n",
    "knn_imp = build_imputer( KNeighborsRegressor(n_jobs=4) ,scaler.transform(X_train),Y_train,transform_x=False,reset_index = True,verbose=True,max_iter=64,tol=0.35,scaler=scaler)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "simple_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "simple_imp = simple_imp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.1873817422860384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:52:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.13694807 0.13694807        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:06:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.21135067 0.21110301        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 2.310129700083158e-09}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:08:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.19189482 0.19189482        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 2.848035868435799e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 256}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:25:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "Simple & No & Acc & 0.95 & 0.95 & 0.94 & 0.95 & 0.07 & 0.96 & 0.95 & 0.96 & 0.97 & 0.97 \\\\\n",
      "~ & ~ & Prec & 0.06 & 0.00 & 0.00 & 0.00 & 0.05 & 0.66 & 0.00 & 0.79 & 0.87 & 1.00 \\\\\n",
      "~ & ~ & Rec & 0.01 & 0.00 & 0.00 & 0.00 & 0.99 & 0.42 & 0.00 & 0.34 & 0.45 & 0.31 \\\\\n",
      "~ & ~ & F1 & 0.01 & 0.00 & 0.00 & 0.00 & 0.09 & 0.51 & 0.00 & 0.48 & 0.59 & 0.47 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.85 & 0.92 & 0.81 & 0.80 & 0.06 & 0.93 & 0.86 & 0.96 & 0.96 & 0.96 \\\\\n",
      "~ & ~ & Prec & 0.16 & 0.15 & 0.11 & 0.10 & 0.05 & 0.35 & 0.18 & 0.54 & 0.61 & 0.62 \\\\\n",
      "~ & ~ & Rec & 0.49 & 0.15 & 0.40 & 0.43 & 0.99 & 0.51 & 0.52 & 0.45 & 0.53 & 0.45 \\\\\n",
      "~ & ~ & F1 & 0.24 & 0.15 & 0.17 & 0.17 & 0.09 & 0.42 & 0.26 & 0.49 & 0.57 & 0.52 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.88 & 0.87 & 0.77 & 0.80 & 0.12 & 0.92 & 0.85 & 0.89 & 0.89 & 0.93 \\\\\n",
      "~ & ~ & Prec & 0.23 & 0.14 & 0.10 & 0.11 & 0.05 & 0.32 & 0.11 & 0.26 & 0.29 & 0.37 \\\\\n",
      "~ & ~ & Rec & 0.64 & 0.33 & 0.49 & 0.46 & 0.96 & 0.57 & 0.33 & 0.75 & 0.82 & 0.68 \\\\\n",
      "~ & ~ & F1 & 0.34 & 0.19 & 0.17 & 0.18 & 0.09 & 0.41 & 0.17 & 0.39 & 0.42 & 0.48 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.74 & 0.83 & 0.74 & 0.72 & 0.11 & 0.89 & 0.74 & 0.95 & 0.96 & 0.94 \\\\\n",
      "~ & ENN & Prec & 0.12 & 0.12 & 0.09 & 0.09 & 0.05 & 0.23 & 0.12 & 0.44 & 0.54 & 0.39 \\\\\n",
      "~ & ~ & Rec & 0.71 & 0.41 & 0.49 & 0.57 & 0.94 & 0.61 & 0.68 & 0.47 & 0.59 & 0.62 \\\\\n",
      "~ & ~ & F1 & 0.20 & 0.18 & 0.15 & 0.16 & 0.09 & 0.33 & 0.20 & 0.45 & 0.57 & 0.48 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"Simple & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9473886328725039   |   0.0625   |   0.008130081300813009   |   0.014388489208633094   |  \n",
      "LDA   |   0.9481566820276498   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-5   |   0.9443164362519201   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.07219662058371736   |   0.04808829325975562   |   0.991869918699187   |   0.09172932330827067   |  \n",
      "DT   |   0.9623655913978495   |   0.6582278481012658   |   0.42276422764227645   |   0.5148514851485149   |  \n",
      "SVC   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9646697388632872   |   0.7924528301886793   |   0.34146341463414637   |   0.4772727272727273   |  \n",
      "XGB   |   0.9708141321044547   |   0.873015873015873   |   0.44715447154471544   |   0.5913978494623655   |  \n",
      "Voting   |   0.967357910906298   |   1.0   |   0.3089430894308943   |   0.4720496894409938   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8517665130568356   |   0.1566579634464752   |   0.4878048780487805   |   0.23715415019762848   |  \n",
      "LDA   |   0.9212749615975423   |   0.15254237288135594   |   0.14634146341463414   |   0.14937759336099585   |  \n",
      "KNN-5   |   0.8114439324116743   |   0.10515021459227468   |   0.3983739837398374   |   0.16638370118845502   |  \n",
      "KNN-10   |   0.7956989247311828   |   0.1029126213592233   |   0.43089430894308944   |   0.16614420062695923   |  \n",
      "GNB   |   0.059907834101382486   |   0.047489295445698715   |   0.991869918699187   |   0.09063893016344723   |  \n",
      "DT   |   0.9324116743471582   |   0.35195530726256985   |   0.5121951219512195   |   0.41721854304635764   |  \n",
      "SVC   |   0.8632872503840245   |   0.1772853185595568   |   0.5203252032520326   |   0.2644628099173554   |  \n",
      "RFC   |   0.956221198156682   |   0.5445544554455446   |   0.44715447154471544   |   0.4910714285714286   |  \n",
      "XGB   |   0.9619815668202765   |   0.6132075471698113   |   0.5284552845528455   |   0.5676855895196506   |  \n",
      "Voting   |   0.9612135176651305   |   0.625   |   0.44715447154471544   |   0.5213270142180094   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8828725038402457   |   0.2323529411764706   |   0.6422764227642277   |   0.3412526997840173   |  \n",
      "LDA   |   0.8701996927803379   |   0.13559322033898305   |   0.3252032520325203   |   0.19138755980861244   |  \n",
      "KNN-5   |   0.7707373271889401   |   0.10101010101010101   |   0.4878048780487805   |   0.1673640167364017   |  \n",
      "KNN-10   |   0.803763440860215   |   0.11354581673306773   |   0.4634146341463415   |   0.1824   |  \n",
      "GNB   |   0.11943164362519201   |   0.049044056525353284   |   0.959349593495935   |   0.0933175168050613   |  \n",
      "DT   |   0.9216589861751152   |   0.3167420814479638   |   0.5691056910569106   |   0.4069767441860465   |  \n",
      "SVC   |   0.84715821812596   |   0.11484593837535013   |   0.3333333333333333   |   0.17083333333333334   |  \n",
      "RFC   |   0.8882488479262672   |   0.26136363636363635   |   0.7479674796747967   |   0.38736842105263153   |  \n",
      "XGB   |   0.8947772657450077   |   0.28611898016997167   |   0.8211382113821138   |   0.4243697478991597   |  \n",
      "Voting   |   0.9297235023041475   |   0.3684210526315789   |   0.6829268292682927   |   0.47863247863247865   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.7373271889400922   |   0.11836734693877551   |   0.7073170731707317   |   0.2027972027972028   |  \n",
      "LDA   |   0.825652841781874   |   0.11600928074245939   |   0.4065040650406504   |   0.18050541516245486   |  \n",
      "KNN-5   |   0.7400153609831029   |   0.08902077151335312   |   0.4878048780487805   |   0.150564617314931   |  \n",
      "KNN-10   |   0.7208141321044547   |   0.09408602150537634   |   0.5691056910569106   |   0.16147635524798154   |  \n",
      "GNB   |   0.11328725038402458   |   0.04797353184449959   |   0.943089430894309   |   0.09130263675718221   |  \n",
      "DT   |   0.8851766513056836   |   0.23006134969325154   |   0.6097560975609756   |   0.33407572383073497   |  \n",
      "SVC   |   0.7419354838709677   |   0.11715481171548117   |   0.6829268292682927   |   0.2   |  \n",
      "RFC   |   0.9466205837173579   |   0.4393939393939394   |   0.4715447154471545   |   0.4549019607843138   |  \n",
      "XGB   |   0.956989247311828   |   0.5407407407407407   |   0.5934959349593496   |   0.5658914728682171   |  \n",
      "Voting   |   0.9358678955453149   |   0.3877551020408163   |   0.6178861788617886   |   0.4764890282131662   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MissForest Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.01399797 0.01399797        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.004328761281083057}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:39:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.11230456 0.11230456        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.3511191734215131}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:51:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.1620403  0.16254906        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'lsqr'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.232846739442066e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[19:53:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 251.18864315095823, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.16587505 0.16587505        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1e-09}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[20:04:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "MissForest & No & Acc & 0.95 & 0.95 & 0.95 & 0.95 & 0.08 & 0.93 & 0.95 & 0.95 & 0.96 & 0.95 \\\\\n",
      "~ & ~ & Prec & 0.05 & 0.05 & 0.00 & 0.00 & 0.05 & 0.20 & 0.00 & 0.08 & 0.82 & 0.50 \\\\\n",
      "~ & ~ & Rec & 0.01 & 0.01 & 0.00 & 0.00 & 1.00 & 0.14 & 0.00 & 0.01 & 0.15 & 0.01 \\\\\n",
      "~ & ~ & F1 & 0.01 & 0.01 & 0.00 & 0.00 & 0.09 & 0.17 & 0.00 & 0.01 & 0.25 & 0.02 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.86 & 0.93 & 0.81 & 0.79 & 0.06 & 0.79 & 0.86 & 0.92 & 0.95 & 0.93 \\\\\n",
      "~ & ~ & Prec & 0.15 & 0.11 & 0.10 & 0.10 & 0.05 & 0.11 & 0.11 & 0.28 & 0.46 & 0.26 \\\\\n",
      "~ & ~ & Rec & 0.45 & 0.08 & 0.37 & 0.44 & 0.99 & 0.50 & 0.29 & 0.40 & 0.30 & 0.33 \\\\\n",
      "~ & ~ & F1 & 0.23 & 0.09 & 0.15 & 0.17 & 0.09 & 0.19 & 0.16 & 0.33 & 0.36 & 0.29 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.87 & 0.89 & 0.75 & 0.79 & 0.40 & 0.70 & 0.85 & 0.83 & 0.84 & 0.86 \\\\\n",
      "~ & ~ & Prec & 0.14 & 0.10 & 0.09 & 0.10 & 0.06 & 0.10 & 0.11 & 0.16 & 0.18 & 0.20 \\\\\n",
      "~ & ~ & Rec & 0.36 & 0.19 & 0.48 & 0.44 & 0.77 & 0.69 & 0.30 & 0.65 & 0.67 & 0.63 \\\\\n",
      "~ & ~ & F1 & 0.21 & 0.13 & 0.16 & 0.16 & 0.11 & 0.18 & 0.16 & 0.26 & 0.29 & 0.30 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.76 & 0.86 & 0.74 & 0.72 & 0.12 & 0.82 & 0.77 & 0.88 & 0.94 & 0.88 \\\\\n",
      "~ & ENN & Prec & 0.13 & 0.12 & 0.08 & 0.09 & 0.05 & 0.12 & 0.12 & 0.17 & 0.38 & 0.21 \\\\\n",
      "~ & ~ & Rec & 0.69 & 0.33 & 0.46 & 0.50 & 0.98 & 0.44 & 0.63 & 0.37 & 0.42 & 0.54 \\\\\n",
      "~ & ~ & F1 & 0.21 & 0.18 & 0.14 & 0.15 & 0.10 & 0.19 & 0.20 & 0.23 & 0.40 & 0.31 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"MissForest & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.946236559139785   |   0.05263157894736842   |   0.008130081300813009   |   0.014084507042253521   |  \n",
      "LDA   |   0.945468509984639   |   0.047619047619047616   |   0.008130081300813009   |   0.013888888888888888   |  \n",
      "KNN-5   |   0.946236559139785   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.08179723502304148   |   0.04892601431980907   |   1.0   |   0.09328782707622298   |  \n",
      "DT   |   0.9339477726574501   |   0.20481927710843373   |   0.13821138211382114   |   0.1650485436893204   |  \n",
      "SVC   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9489247311827957   |   0.08333333333333333   |   0.008130081300813009   |   0.014814814814814815   |  \n",
      "XGB   |   0.9581413210445469   |   0.8181818181818182   |   0.14634146341463414   |   0.2482758620689655   |  \n",
      "Voting   |   0.9527649769585254   |   0.5   |   0.008130081300813009   |   0.016   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8567588325652842   |   0.1527777777777778   |   0.44715447154471544   |   0.22774327122153212   |  \n",
      "LDA   |   0.9262672811059908   |   0.11235955056179775   |   0.08130081300813008   |   0.09433962264150944   |  \n",
      "KNN-5   |   0.8064516129032258   |   0.09725158562367865   |   0.37398373983739835   |   0.15436241610738255   |  \n",
      "KNN-10   |   0.793778801843318   |   0.10344827586206896   |   0.43902439024390244   |   0.16744186046511625   |  \n",
      "GNB   |   0.06336405529953917   |   0.04765625   |   0.991869918699187   |   0.09094297428251956   |  \n",
      "DT   |   0.793778801843318   |   0.11380597014925373   |   0.4959349593495935   |   0.18512898330804248   |  \n",
      "SVC   |   0.8567588325652842   |   0.11180124223602485   |   0.2926829268292683   |   0.16179775280898878   |  \n",
      "RFC   |   0.924347158218126   |   0.28488372093023256   |   0.3983739837398374   |   0.3322033898305085   |  \n",
      "XGB   |   0.9500768049155146   |   0.4567901234567901   |   0.3008130081300813   |   0.36274509803921573   |  \n",
      "Voting   |   0.9251152073732719   |   0.2631578947368421   |   0.3252032520325203   |   0.2909090909090909   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.869431643625192   |   0.14426229508196722   |   0.35772357723577236   |   0.20560747663551404   |  \n",
      "LDA   |   0.8851766513056836   |   0.1036036036036036   |   0.18699186991869918   |   0.13333333333333333   |  \n",
      "KNN-5   |   0.7549923195084486   |   0.09320695102685624   |   0.4796747967479675   |   0.15608465608465608   |  \n",
      "KNN-10   |   0.7872503840245776   |   0.10018552875695733   |   0.43902439024390244   |   0.1631419939577039   |  \n",
      "GNB   |   0.40053763440860213   |   0.05835380835380835   |   0.7723577235772358   |   0.10850942318675043   |  \n",
      "DT   |   0.7000768049155146   |   0.10265700483091787   |   0.6910569105691057   |   0.17875920084121974   |  \n",
      "SVC   |   0.8548387096774194   |   0.11246200607902736   |   0.3008130081300813   |   0.16371681415929204   |  \n",
      "RFC   |   0.826036866359447   |   0.16326530612244897   |   0.6504065040650406   |   0.26101141924959215   |  \n",
      "XGB   |   0.8417818740399385   |   0.1824175824175824   |   0.6747967479674797   |   0.28719723183391   |  \n",
      "Voting   |   0.8621351766513057   |   0.19743589743589743   |   0.6260162601626016   |   0.3001949317738791   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.7576804915514593   |   0.12536873156342182   |   0.6910569105691057   |   0.21223470661672908   |  \n",
      "LDA   |   0.8605990783410138   |   0.125   |   0.3252032520325203   |   0.18058690744920994   |  \n",
      "KNN-5   |   0.7380952380952381   |   0.08469539375928678   |   0.4634146341463415   |   0.14321608040201006   |  \n",
      "KNN-10   |   0.7215821812596006   |   0.08539944903581267   |   0.5040650406504065   |   0.14605418138987045   |  \n",
      "GNB   |   0.12096774193548387   |   0.05024916943521595   |   0.983739837398374   |   0.09561438166732517   |  \n",
      "DT   |   0.8241167434715821   |   0.12189616252821671   |   0.43902439024390244   |   0.19081272084805653   |  \n",
      "SVC   |   0.7680491551459293   |   0.12125984251968504   |   0.6260162601626016   |   0.20316622691292874   |  \n",
      "RFC   |   0.8817204301075269   |   0.16606498194945848   |   0.37398373983739835   |   0.23   |  \n",
      "XGB   |   0.9397081413210445   |   0.37681159420289856   |   0.42276422764227645   |   0.3984674329501916   |  \n",
      "Voting   |   0.8828725038402457   |   0.2120253164556962   |   0.5447154471544715   |   0.3052391799544419   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7812, 49)\n",
      "[IterativeImputer] Completing matrix with shape (2604, 49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.0277091 0.0277091       nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.1873817422860384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[20:17:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.11817452 0.11817452        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 11}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 256}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[20:30:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.16161013 0.1615129         nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 4.3287612810830526e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[20:32:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 5.336699231206313e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 20}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[20:44:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 2,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "KNN & No & Acc & 0.95 & 0.95 & 0.94 & 0.95 & 0.07 & 0.94 & 0.95 & 0.95 & 0.96 & 0.95 \\\\\n",
      "~ & ~ & Prec & 0.05 & 0.00 & 0.00 & 0.00 & 0.05 & 0.21 & 0.00 & 0.07 & 0.83 & 0.00 \\\\\n",
      "~ & ~ & Rec & 0.01 & 0.00 & 0.00 & 0.00 & 0.99 & 0.14 & 0.00 & 0.01 & 0.12 & 0.00 \\\\\n",
      "~ & ~ & F1 & 0.01 & 0.00 & 0.00 & 0.00 & 0.09 & 0.17 & 0.00 & 0.01 & 0.21 & 0.00 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.85 & 0.92 & 0.80 & 0.78 & 0.06 & 0.85 & 0.85 & 0.91 & 0.95 & 0.92 \\\\\n",
      "~ & ~ & Prec & 0.14 & 0.14 & 0.10 & 0.10 & 0.05 & 0.13 & 0.12 & 0.22 & 0.45 & 0.23 \\\\\n",
      "~ & ~ & Rec & 0.44 & 0.14 & 0.40 & 0.44 & 0.99 & 0.35 & 0.37 & 0.33 & 0.37 & 0.26 \\\\\n",
      "~ & ~ & F1 & 0.22 & 0.14 & 0.16 & 0.16 & 0.09 & 0.19 & 0.19 & 0.26 & 0.40 & 0.24 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.85 & 0.88 & 0.76 & 0.79 & 0.10 & 0.82 & 0.85 & 0.80 & 0.83 & 0.84 \\\\\n",
      "~ & ~ & Prec & 0.15 & 0.11 & 0.10 & 0.11 & 0.05 & 0.10 & 0.11 & 0.15 & 0.17 & 0.17 \\\\\n",
      "~ & ~ & Rec & 0.46 & 0.22 & 0.50 & 0.46 & 0.97 & 0.34 & 0.33 & 0.66 & 0.70 & 0.58 \\\\\n",
      "~ & ~ & F1 & 0.22 & 0.15 & 0.16 & 0.17 & 0.09 & 0.15 & 0.17 & 0.24 & 0.28 & 0.26 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.74 & 0.83 & 0.73 & 0.71 & 0.11 & 0.84 & 0.75 & 0.91 & 0.91 & 0.91 \\\\\n",
      "~ & ENN & Prec & 0.12 & 0.12 & 0.09 & 0.09 & 0.05 & 0.13 & 0.12 & 0.21 & 0.28 & 0.21 \\\\\n",
      "~ & ~ & Rec & 0.70 & 0.40 & 0.49 & 0.55 & 0.94 & 0.41 & 0.67 & 0.35 & 0.53 & 0.36 \\\\\n",
      "~ & ~ & F1 & 0.20 & 0.18 & 0.14 & 0.15 & 0.09 & 0.20 & 0.20 & 0.27 & 0.36 & 0.27 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"KNN & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.946236559139785   |   0.05263157894736842   |   0.008130081300813009   |   0.014084507042253521   |  \n",
      "LDA   |   0.9481566820276498   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-5   |   0.9439324116743472   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.07373271889400922   |   0.048164232135807346   |   0.991869918699187   |   0.09186746987951806   |  \n",
      "DT   |   0.935099846390169   |   0.2125   |   0.13821138211382114   |   0.16748768472906403   |  \n",
      "SVC   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9481566820276498   |   0.07142857142857142   |   0.008130081300813009   |   0.014598540145985401   |  \n",
      "XGB   |   0.9573732718894009   |   0.8333333333333334   |   0.12195121951219512   |   0.21276595744680848   |  \n",
      "Voting   |   0.9527649769585254   |   0.0   |   0.0   |   0.0   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8506144393241167   |   0.1443850267379679   |   0.43902439024390244   |   0.21730382293762576   |  \n",
      "LDA   |   0.9201228878648233   |   0.14285714285714285   |   0.13821138211382114   |   0.14049586776859505   |  \n",
      "KNN-5   |   0.7983870967741935   |   0.098   |   0.3983739837398374   |   0.15730337078651685   |  \n",
      "KNN-10   |   0.7818740399385561   |   0.09764918625678119   |   0.43902439024390244   |   0.15976331360946747   |  \n",
      "GNB   |   0.06029185867895545   |   0.04750778816199377   |   0.991869918699187   |   0.09067261241174285   |  \n",
      "DT   |   0.8548387096774194   |   0.12609970674486803   |   0.34959349593495936   |   0.18534482758620688   |  \n",
      "SVC   |   0.8490783410138248   |   0.125   |   0.36585365853658536   |   0.18633540372670807   |  \n",
      "RFC   |   0.9139784946236559   |   0.22099447513812154   |   0.3252032520325203   |   0.2631578947368421   |  \n",
      "XGB   |   0.9489247311827957   |   0.45   |   0.36585365853658536   |   0.40358744394618834   |  \n",
      "Voting   |   0.9231950844854071   |   0.22695035460992907   |   0.2601626016260163   |   0.24242424242424243   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.848310291858679   |   0.14583333333333334   |   0.45528455284552843   |   0.22090729783037477   |  \n",
      "LDA   |   0.8824884792626728   |   0.11392405063291139   |   0.21951219512195122   |   0.14999999999999997   |  \n",
      "KNN-5   |   0.761520737327189   |   0.09838709677419355   |   0.4959349593495935   |   0.1641991924629879   |  \n",
      "KNN-10   |   0.7910906298003072   |   0.10654205607476636   |   0.4634146341463415   |   0.17325227963525835   |  \n",
      "GNB   |   0.10138248847926268   |   0.048472505091649694   |   0.967479674796748   |   0.09231962761830877   |  \n",
      "DT   |   0.8206605222734255   |   0.09813084112149532   |   0.34146341463414637   |   0.15245009074410162   |  \n",
      "SVC   |   0.8456221198156681   |   0.11357340720221606   |   0.3333333333333333   |   0.1694214876033058   |  \n",
      "RFC   |   0.8029953917050692   |   0.14673913043478262   |   0.6585365853658537   |   0.24000000000000002   |  \n",
      "XGB   |   0.8287250384024577   |   0.17373737373737375   |   0.6991869918699187   |   0.2783171521035599   |  \n",
      "Voting   |   0.8425499231950845   |   0.1655011655011655   |   0.5772357723577236   |   0.25724637681159424   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.738479262672811   |   0.1178082191780822   |   0.6991869918699187   |   0.20164126611957797   |  \n",
      "LDA   |   0.8279569892473119   |   0.11583924349881797   |   0.3983739837398374   |   0.1794871794871795   |  \n",
      "KNN-5   |   0.728110599078341   |   0.0851063829787234   |   0.4878048780487805   |   0.14492753623188406   |  \n",
      "KNN-10   |   0.7085253456221198   |   0.08808290155440414   |   0.5528455284552846   |   0.15195530726256984   |  \n",
      "GNB   |   0.11175115207373272   |   0.04789430222956235   |   0.943089430894309   |   0.09115913555992142   |  \n",
      "DT   |   0.8402457757296466   |   0.1291139240506329   |   0.4146341463414634   |   0.1969111969111969   |  \n",
      "SVC   |   0.7450076804915514   |   0.1173974540311174   |   0.6747967479674797   |   0.2   |  \n",
      "RFC   |   0.9089861751152074   |   0.215   |   0.34959349593495936   |   0.26625386996904027   |  \n",
      "XGB   |   0.9120583717357911   |   0.2754237288135593   |   0.5284552845528455   |   0.362116991643454   |  \n",
      "Voting   |   0.9070660522273426   |   0.21256038647342995   |   0.35772357723577236   |   0.26666666666666666   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------\n",
    "# EOF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
