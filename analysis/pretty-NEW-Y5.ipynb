{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install scikit-learn==0.20.1  # dep for missingpy\n",
    "#!pip3 install scikit-learn==0.24.1  # required for getting tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year $N$\n",
    "Run all in this section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = '5'\n",
    "drop_cols = ['Attr37', 'Attr7','Attr43','Attr62','Attr32','Attr44','Attr15','Attr19','Attr3','Attr51','Attr4','Attr49','Attr38','Attr60','Attr6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff = loadarff(data_path+N+'year.arff')\n",
    "df = pd.DataFrame(arff[0])\n",
    "df['class']= df['class'].astype('int')\n",
    "df = df.drop_duplicates()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attr37    2525\n",
       "Attr27     390\n",
       "Attr60     266\n",
       "Attr45     266\n",
       "Attr24     135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change dropcols accordingly...\n",
    "df.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(drop_cols,axis='columns')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain NaN in test set also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4081\n",
       "1     306\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1361\n",
       "1     102\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']\n",
    "# (X.shape,Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=random_state,shuffle=True,stratify=Y)\n",
    "\n",
    "display(Y_train.value_counts(), Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test,Y_test = X_train.to_numpy(),Y_train.to_numpy(),X_test.to_numpy(),Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "#scaler = None\n",
    "\n",
    "#idx = np.isnan(X_train).any(axis=1)\n",
    "#scaler = StandardScaler().fit(X_train[~idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_imputer(imputer_estimator,X_train,Y_train,transform_x=True,reset_index = True,verbose=True,max_iter=10,tol=1,imputer=None,scaler=None):\n",
    "    # train = pd.concat([X_train,Y_train],axis=1)\n",
    "    # train['class'] = train['class'].astype('category')\n",
    "    # if reset_index:\n",
    "    #     train = train.reset_index(drop=True)\n",
    "    \n",
    "    if imputer is None:\n",
    "        imputer = IterativeImputer(estimator=imputer_estimator, n_nearest_features=None, imputation_order='descending',verbose=verbose,max_iter=max_iter,tol=tol)\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "    else:\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "        \n",
    "    \n",
    "    if transform_x:\n",
    "        X_train = imputer.transform(X_train)\n",
    "        \n",
    "        #if scaler is not None:\n",
    "        #    X_train = scaler.transform(X_train)\n",
    "        \n",
    "        return imputer,X_train\n",
    "    \n",
    "    return imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom missingpy import MissForest\\n\\nmissf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\\nmissf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\\nX_test_imp = missf_imp.transform(X_test)\\n\\n# note... these are not scaled!!\\nnp.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\\nnp.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from missingpy import MissForest\n",
    "\n",
    "missf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\n",
    "missf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\n",
    "X_test_imp = missf_imp.transform(X_test)\n",
    "\n",
    "# note... these are not scaled!!\n",
    "np.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\n",
    "np.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (4387, 49)\n",
      "[IterativeImputer] Change: 12.974082274248358, scaled tolerance: 13.219389217534895 \n",
      "[IterativeImputer] Early stopping criterion reached.\n"
     ]
    }
   ],
   "source": [
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# has to be scaled...\n",
    "knn_imp = build_imputer( KNeighborsRegressor(n_jobs=4) ,scaler.transform(X_train),Y_train,transform_x=False,reset_index = True,verbose=True,max_iter=64,tol=0.2,scaler=scaler)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "simple_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "simple_imp = simple_imp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.25211023 0.25211023        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 8.111308307896872e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[17:39:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.37031616 0.36976861        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.04328761281083057}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[17:46:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.37444444 0.37444444        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 5.336699231206313e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[17:48:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.37938063 0.37938063        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 8.111308307896873e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[17:53:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 2,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "Simple & No & Acc & 0.93 & 0.93 & 0.93 & 0.93 & 0.92 & 0.96 & 0.93 & 0.95 & 0.96 & 0.96 \\\\\n",
      "~ & ~ & Prec & 0.56 & 0.39 & 0.62 & 0.56 & 0.32 & 0.85 & 0.00 & 0.81 & 0.94 & 0.97 \\\\\n",
      "~ & ~ & Rec & 0.19 & 0.07 & 0.15 & 0.15 & 0.07 & 0.55 & 0.00 & 0.41 & 0.49 & 0.38 \\\\\n",
      "~ & ~ & F1 & 0.28 & 0.12 & 0.24 & 0.23 & 0.11 & 0.67 & 0.00 & 0.55 & 0.65 & 0.55 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.89 & 0.89 & 0.84 & 0.85 & 0.93 & 0.91 & 0.90 & 0.94 & 0.95 & 0.94 \\\\\n",
      "~ & ~ & Prec & 0.31 & 0.31 & 0.21 & 0.23 & 0.40 & 0.41 & 0.34 & 0.55 & 0.63 & 0.58 \\\\\n",
      "~ & ~ & Rec & 0.54 & 0.44 & 0.47 & 0.52 & 0.06 & 0.74 & 0.53 & 0.60 & 0.66 & 0.60 \\\\\n",
      "~ & ~ & F1 & 0.40 & 0.36 & 0.29 & 0.32 & 0.10 & 0.53 & 0.42 & 0.57 & 0.64 & 0.59 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.86 & 0.88 & 0.86 & 0.90 & 0.92 & 0.89 & 0.89 & 0.87 & 0.91 & 0.91 \\\\\n",
      "~ & ~ & Prec & 0.27 & 0.28 & 0.28 & 0.34 & 0.29 & 0.38 & 0.31 & 0.32 & 0.42 & 0.43 \\\\\n",
      "~ & ~ & Rec & 0.55 & 0.49 & 0.59 & 0.51 & 0.12 & 0.77 & 0.49 & 0.74 & 0.76 & 0.74 \\\\\n",
      "~ & ~ & F1 & 0.36 & 0.36 & 0.38 & 0.41 & 0.17 & 0.51 & 0.38 & 0.45 & 0.54 & 0.54 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.84 & 0.85 & 0.81 & 0.82 & 0.92 & 0.92 & 0.84 & 0.92 & 0.93 & 0.91 \\\\\n",
      "~ & ENN & Prec & 0.26 & 0.24 & 0.20 & 0.21 & 0.34 & 0.44 & 0.25 & 0.46 & 0.49 & 0.41 \\\\\n",
      "~ & ~ & Rec & 0.68 & 0.52 & 0.57 & 0.59 & 0.20 & 0.69 & 0.67 & 0.55 & 0.69 & 0.67 \\\\\n",
      "~ & ~ & F1 & 0.38 & 0.33 & 0.29 & 0.31 & 0.25 & 0.53 & 0.36 & 0.50 & 0.57 & 0.51 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"Simple & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9330143540669856   |   0.5588235294117647   |   0.18627450980392157   |   0.27941176470588236   |  \n",
      "LDA   |   0.9275461380724539   |   0.3888888888888889   |   0.06862745098039216   |   0.11666666666666667   |  \n",
      "KNN-5   |   0.9343814080656186   |   0.625   |   0.14705882352941177   |   0.2380952380952381   |  \n",
      "KNN-10   |   0.9323308270676691   |   0.5555555555555556   |   0.14705882352941177   |   0.23255813953488372   |  \n",
      "GNB   |   0.924812030075188   |   0.3181818181818182   |   0.06862745098039216   |   0.11290322580645162   |  \n",
      "DT   |   0.9617224880382775   |   0.8484848484848485   |   0.5490196078431373   |   0.6666666666666667   |  \n",
      "SVC   |   0.9282296650717703   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9521531100478469   |   0.8076923076923077   |   0.4117647058823529   |   0.5454545454545454   |  \n",
      "XGB   |   0.9624060150375939   |   0.9433962264150944   |   0.49019607843137253   |   0.6451612903225806   |  \n",
      "Voting   |   0.9562542720437457   |   0.975   |   0.38235294117647056   |   0.5492957746478873   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.885850991114149   |   0.3142857142857143   |   0.5392156862745098   |   0.3971119133574007   |  \n",
      "LDA   |   0.8920027341079972   |   0.3082191780821918   |   0.4411764705882353   |   0.36290322580645157   |  \n",
      "KNN-5   |   0.8393711551606289   |   0.2096069868995633   |   0.47058823529411764   |   0.29003021148036257   |  \n",
      "KNN-10   |   0.8462064251537936   |   0.2314410480349345   |   0.5196078431372549   |   0.32024169184290036   |  \n",
      "GNB   |   0.9282296650717703   |   0.4   |   0.058823529411764705   |   0.10256410256410256   |  \n",
      "DT   |   0.9077238550922762   |   0.4098360655737705   |   0.7352941176470589   |   0.5263157894736843   |  \n",
      "SVC   |   0.8961038961038961   |   0.34177215189873417   |   0.5294117647058824   |   0.4153846153846154   |  \n",
      "RFC   |   0.937799043062201   |   0.5495495495495496   |   0.5980392156862745   |   0.5727699530516432   |  \n",
      "XGB   |   0.9487354750512645   |   0.6261682242990654   |   0.6568627450980392   |   0.6411483253588517   |  \n",
      "Voting   |   0.9412166780587833   |   0.5754716981132075   |   0.5980392156862745   |   0.5865384615384615   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8632946001367054   |   0.26666666666666666   |   0.5490196078431373   |   0.358974358974359   |  \n",
      "LDA   |   0.8762816131237184   |   0.27932960893854747   |   0.49019607843137253   |   0.3558718861209964   |  \n",
      "KNN-5   |   0.8646616541353384   |   0.2777777777777778   |   0.5882352941176471   |   0.3773584905660377   |  \n",
      "KNN-10   |   0.8981544771018455   |   0.3443708609271523   |   0.5098039215686274   |   0.4110671936758893   |  \n",
      "GNB   |   0.9179767600820232   |   0.2857142857142857   |   0.11764705882352941   |   0.16666666666666666   |  \n",
      "DT   |   0.8947368421052632   |   0.3761904761904762   |   0.7745098039215687   |   0.5064102564102565   |  \n",
      "SVC   |   0.8879015721120984   |   0.30864197530864196   |   0.49019607843137253   |   0.3787878787878788   |  \n",
      "RFC   |   0.8735475051264525   |   0.3218884120171674   |   0.7352941176470589   |   0.4477611940298507   |  \n",
      "XGB   |   0.9097744360902256   |   0.41935483870967744   |   0.7647058823529411   |   0.5416666666666666   |  \n",
      "Voting   |   0.9131920710868079   |   0.42857142857142855   |   0.7352941176470589   |   0.5415162454873645   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8441558441558441   |   0.26136363636363635   |   0.6764705882352942   |   0.3770491803278689   |  \n",
      "LDA   |   0.8503075871496925   |   0.23766816143497757   |   0.5196078431372549   |   0.3261538461538461   |  \n",
      "KNN-5   |   0.8099794941900205   |   0.19863013698630136   |   0.5686274509803921   |   0.29441624365482233   |  \n",
      "KNN-10   |   0.8181818181818182   |   0.2112676056338028   |   0.5882352941176471   |   0.31088082901554404   |  \n",
      "GNB   |   0.9179767600820232   |   0.3448275862068966   |   0.19607843137254902   |   0.25   |  \n",
      "DT   |   0.9166097060833903   |   0.4375   |   0.6862745098039216   |   0.5343511450381679   |  \n",
      "SVC   |   0.836637047163363   |   0.2490842490842491   |   0.6666666666666666   |   0.36266666666666664   |  \n",
      "RFC   |   0.9234449760765551   |   0.45901639344262296   |   0.5490196078431373   |   0.5   |  \n",
      "XGB   |   0.9289131920710868   |   0.49295774647887325   |   0.6862745098039216   |   0.5737704918032787   |  \n",
      "Voting   |   0.9104579630895421   |   0.4121212121212121   |   0.6666666666666666   |   0.5093632958801498   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MissForest Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[17:59:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 3.511191734215127e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:06:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 2,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.36770026 0.36728478        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 8.111308307896872e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:09:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.38551421 0.38551421        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 2.848035868435799e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:15:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "MissForest & No & Acc & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.94 & 0.95 & 0.94 \\\\\n",
      "~ & ~ & Prec & 0.54 & 0.40 & 0.56 & 0.56 & 0.33 & 0.46 & 0.25 & 0.64 & 0.80 & 0.72 \\\\\n",
      "~ & ~ & Rec & 0.19 & 0.08 & 0.14 & 0.15 & 0.07 & 0.42 & 0.01 & 0.23 & 0.35 & 0.23 \\\\\n",
      "~ & ~ & F1 & 0.28 & 0.13 & 0.22 & 0.23 & 0.11 & 0.44 & 0.02 & 0.33 & 0.49 & 0.34 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.89 & 0.90 & 0.84 & 0.84 & 0.93 & 0.87 & 0.90 & 0.93 & 0.94 & 0.93 \\\\\n",
      "~ & ~ & Prec & 0.32 & 0.34 & 0.20 & 0.22 & 0.33 & 0.28 & 0.35 & 0.47 & 0.57 & 0.51 \\\\\n",
      "~ & ~ & Rec & 0.56 & 0.43 & 0.44 & 0.52 & 0.06 & 0.60 & 0.52 & 0.55 & 0.49 & 0.58 \\\\\n",
      "~ & ~ & F1 & 0.41 & 0.38 & 0.27 & 0.31 & 0.10 & 0.38 & 0.42 & 0.51 & 0.53 & 0.54 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.88 & 0.88 & 0.86 & 0.90 & 0.92 & 0.88 & 0.89 & 0.87 & 0.87 & 0.90 \\\\\n",
      "~ & ~ & Prec & 0.30 & 0.28 & 0.27 & 0.35 & 0.27 & 0.32 & 0.33 & 0.31 & 0.30 & 0.37 \\\\\n",
      "~ & ~ & Rec & 0.56 & 0.49 & 0.58 & 0.50 & 0.12 & 0.64 & 0.52 & 0.69 & 0.69 & 0.66 \\\\\n",
      "~ & ~ & F1 & 0.39 & 0.36 & 0.37 & 0.41 & 0.16 & 0.43 & 0.40 & 0.43 & 0.42 & 0.48 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.84 & 0.87 & 0.80 & 0.81 & 0.92 & 0.85 & 0.83 & 0.90 & 0.90 & 0.89 \\\\\n",
      "~ & ENN & Prec & 0.26 & 0.29 & 0.18 & 0.20 & 0.39 & 0.27 & 0.24 & 0.36 & 0.36 & 0.34 \\\\\n",
      "~ & ~ & Rec & 0.67 & 0.58 & 0.54 & 0.58 & 0.23 & 0.66 & 0.69 & 0.57 & 0.63 & 0.64 \\\\\n",
      "~ & ~ & F1 & 0.37 & 0.38 & 0.27 & 0.29 & 0.29 & 0.38 & 0.36 & 0.44 & 0.46 & 0.44 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"MissForest & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9323308270676691   |   0.5428571428571428   |   0.18627450980392157   |   0.2773722627737226   |  \n",
      "LDA   |   0.9275461380724539   |   0.4   |   0.0784313725490196   |   0.13114754098360654   |  \n",
      "KNN-5   |   0.9323308270676691   |   0.56   |   0.13725490196078433   |   0.22047244094488191   |  \n",
      "KNN-10   |   0.9323308270676691   |   0.5555555555555556   |   0.14705882352941177   |   0.23255813953488372   |  \n",
      "GNB   |   0.9254955570745045   |   0.3333333333333333   |   0.06862745098039216   |   0.11382113821138212   |  \n",
      "DT   |   0.9254955570745045   |   0.46236559139784944   |   0.4215686274509804   |   0.441025641025641   |  \n",
      "SVC   |   0.9289131920710868   |   0.25   |   0.00980392156862745   |   0.018867924528301886   |  \n",
      "RFC   |   0.9371155160628845   |   0.6388888888888888   |   0.22549019607843138   |   0.3333333333333333   |  \n",
      "XGB   |   0.9487354750512645   |   0.8   |   0.35294117647058826   |   0.489795918367347   |  \n",
      "Voting   |   0.9398496240601504   |   0.71875   |   0.22549019607843138   |   0.3432835820895523   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.885850991114149   |   0.31843575418994413   |   0.5588235294117647   |   0.40569395017793597   |  \n",
      "LDA   |   0.9008885850991114   |   0.33587786259541985   |   0.43137254901960786   |   0.37768240343347637   |  \n",
      "KNN-5   |   0.83526999316473   |   0.1965065502183406   |   0.4411764705882353   |   0.2719033232628399   |  \n",
      "KNN-10   |   0.8393711551606289   |   0.2217573221757322   |   0.5196078431372549   |   0.31085043988269795   |  \n",
      "GNB   |   0.9261790840738209   |   0.3333333333333333   |   0.058823529411764705   |   0.1   |  \n",
      "DT   |   0.8660287081339713   |   0.2824074074074074   |   0.5980392156862745   |   0.3836477987421384   |  \n",
      "SVC   |   0.898838004101162   |   0.34868421052631576   |   0.5196078431372549   |   0.4173228346456692   |  \n",
      "RFC   |   0.9261790840738209   |   0.4745762711864407   |   0.5490196078431373   |   0.5090909090909091   |  \n",
      "XGB   |   0.9391660970608339   |   0.5747126436781609   |   0.49019607843137253   |   0.5291005291005291   |  \n",
      "Voting   |   0.9316473000683527   |   0.5086206896551724   |   0.5784313725490197   |   0.5412844036697249   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8762816131237184   |   0.29533678756476683   |   0.5588235294117647   |   0.3864406779661017   |  \n",
      "LDA   |   0.8783321941216679   |   0.2840909090909091   |   0.49019607843137253   |   0.3597122302158274   |  \n",
      "KNN-5   |   0.8619275461380724   |   0.2706422018348624   |   0.5784313725490197   |   0.36875   |  \n",
      "KNN-10   |   0.9002050580997949   |   0.3493150684931507   |   0.5   |   0.41129032258064513   |  \n",
      "GNB   |   0.9166097060833903   |   0.2727272727272727   |   0.11764705882352941   |   0.1643835616438356   |  \n",
      "DT   |   0.8810663021189337   |   0.3217821782178218   |   0.6372549019607843   |   0.42763157894736836   |  \n",
      "SVC   |   0.8913192071086808   |   0.32515337423312884   |   0.5196078431372549   |   0.4000000000000001   |  \n",
      "RFC   |   0.8721804511278195   |   0.3111111111111111   |   0.6862745098039216   |   0.42813455657492355   |  \n",
      "XGB   |   0.8687628161312372   |   0.30434782608695654   |   0.6862745098039216   |   0.42168674698795183   |  \n",
      "Voting   |   0.8995215311004785   |   0.3743016759776536   |   0.6568627450980392   |   0.4768683274021352   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8421052631578947   |   0.25660377358490566   |   0.6666666666666666   |   0.37057220708446864   |  \n",
      "LDA   |   0.8694463431305537   |   0.28502415458937197   |   0.5784313725490197   |   0.38187702265372164   |  \n",
      "KNN-5   |   0.797676008202324   |   0.18092105263157895   |   0.5392156862745098   |   0.270935960591133   |  \n",
      "KNN-10   |   0.8051948051948052   |   0.19601328903654486   |   0.5784313725490197   |   0.2928039702233251   |  \n",
      "GNB   |   0.9213943950786057   |   0.3898305084745763   |   0.22549019607843138   |   0.2857142857142857   |  \n",
      "DT   |   0.8523581681476419   |   0.2701612903225806   |   0.6568627450980392   |   0.38285714285714284   |  \n",
      "SVC   |   0.8284347231715653   |   0.2422145328719723   |   0.6862745098039216   |   0.35805626598465473   |  \n",
      "RFC   |   0.898838004101162   |   0.35802469135802467   |   0.5686274509803921   |   0.43939393939393934   |  \n",
      "XGB   |   0.8954203691045797   |   0.3575418994413408   |   0.6274509803921569   |   0.4555160142348754   |  \n",
      "Voting   |   0.8872180451127819   |   0.33678756476683935   |   0.6372549019607843   |   0.4406779661016949   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (4387, 49)\n",
      "[IterativeImputer] Completing matrix with shape (1463, 49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.24881451 0.24881451        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 4.3287612810830526e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:20:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 2,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.36023492 0.36023492        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.04328761281083057}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:27:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.38103131 0.38103131        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 8.111308307896873e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:29:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.38478627 0.38478627        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'distance'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.5199110829529332e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[18:36:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "KNN & No & Acc & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.93 & 0.94 & 0.94 \\\\\n",
      "~ & ~ & Prec & 0.57 & 0.39 & 0.54 & 0.58 & 0.33 & 0.46 & 0.25 & 0.56 & 0.67 & 0.67 \\\\\n",
      "~ & ~ & Rec & 0.20 & 0.07 & 0.14 & 0.15 & 0.07 & 0.31 & 0.01 & 0.23 & 0.29 & 0.16 \\\\\n",
      "~ & ~ & F1 & 0.29 & 0.12 & 0.22 & 0.23 & 0.11 & 0.37 & 0.02 & 0.32 & 0.41 & 0.25 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.88 & 0.89 & 0.83 & 0.84 & 0.93 & 0.89 & 0.90 & 0.92 & 0.94 & 0.93 \\\\\n",
      "~ & ~ & Prec & 0.30 & 0.31 & 0.19 & 0.21 & 0.40 & 0.35 & 0.34 & 0.46 & 0.56 & 0.49 \\\\\n",
      "~ & ~ & Rec & 0.54 & 0.45 & 0.43 & 0.49 & 0.06 & 0.62 & 0.52 & 0.57 & 0.49 & 0.54 \\\\\n",
      "~ & ~ & F1 & 0.38 & 0.37 & 0.26 & 0.29 & 0.10 & 0.45 & 0.41 & 0.51 & 0.52 & 0.51 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.86 & 0.87 & 0.86 & 0.90 & 0.92 & 0.84 & 0.89 & 0.87 & 0.87 & 0.88 \\\\\n",
      "~ & ~ & Prec & 0.27 & 0.28 & 0.27 & 0.35 & 0.32 & 0.25 & 0.33 & 0.31 & 0.31 & 0.33 \\\\\n",
      "~ & ~ & Rec & 0.55 & 0.53 & 0.60 & 0.52 & 0.13 & 0.69 & 0.49 & 0.69 & 0.71 & 0.66 \\\\\n",
      "~ & ~ & F1 & 0.36 & 0.36 & 0.37 & 0.42 & 0.18 & 0.37 & 0.39 & 0.42 & 0.43 & 0.44 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.84 & 0.85 & 0.79 & 0.80 & 0.92 & 0.84 & 0.84 & 0.91 & 0.91 & 0.91 \\\\\n",
      "~ & ENN & Prec & 0.27 & 0.24 & 0.18 & 0.19 & 0.36 & 0.24 & 0.25 & 0.40 & 0.41 & 0.39 \\\\\n",
      "~ & ~ & Rec & 0.70 & 0.51 & 0.53 & 0.59 & 0.20 & 0.61 & 0.68 & 0.65 & 0.63 & 0.65 \\\\\n",
      "~ & ~ & F1 & 0.38 & 0.32 & 0.26 & 0.29 & 0.25 & 0.34 & 0.37 & 0.49 & 0.49 & 0.49 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"KNN & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9336978810663021   |   0.5714285714285714   |   0.19607843137254902   |   0.291970802919708   |  \n",
      "LDA   |   0.9275461380724539   |   0.3888888888888889   |   0.06862745098039216   |   0.11666666666666667   |  \n",
      "KNN-5   |   0.9316473000683527   |   0.5384615384615384   |   0.13725490196078433   |   0.21875   |  \n",
      "KNN-10   |   0.9330143540669856   |   0.5769230769230769   |   0.14705882352941177   |   0.23437499999999997   |  \n",
      "GNB   |   0.9254955570745045   |   0.3333333333333333   |   0.06862745098039216   |   0.11382113821138212   |  \n",
      "DT   |   0.9268626110731374   |   0.463768115942029   |   0.3137254901960784   |   0.3742690058479532   |  \n",
      "SVC   |   0.9289131920710868   |   0.25   |   0.00980392156862745   |   0.018867924528301886   |  \n",
      "RFC   |   0.9336978810663021   |   0.5609756097560976   |   0.22549019607843138   |   0.32167832167832167   |  \n",
      "XGB   |   0.9405331510594669   |   0.6666666666666666   |   0.29411764705882354   |   0.40816326530612246   |  \n",
      "Voting   |   0.9357484620642516   |   0.6666666666666666   |   0.1568627450980392   |   0.25396825396825395   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8790157211209843   |   0.2972972972972973   |   0.5392156862745098   |   0.3832752613240418   |  \n",
      "LDA   |   0.8913192071086808   |   0.3087248322147651   |   0.45098039215686275   |   0.3665338645418327   |  \n",
      "KNN-5   |   0.8284347231715653   |   0.18565400843881857   |   0.43137254901960786   |   0.25958702064896755   |  \n",
      "KNN-10   |   0.8359535201640464   |   0.21008403361344538   |   0.49019607843137253   |   0.29411764705882354   |  \n",
      "GNB   |   0.9282296650717703   |   0.4   |   0.058823529411764705   |   0.10256410256410256   |  \n",
      "DT   |   0.8940533151059467   |   0.35195530726256985   |   0.6176470588235294   |   0.4483985765124555   |  \n",
      "SVC   |   0.8954203691045797   |   0.3375796178343949   |   0.5196078431372549   |   0.4092664092664093   |  \n",
      "RFC   |   0.9241285030758715   |   0.464   |   0.5686274509803921   |   0.5110132158590308   |  \n",
      "XGB   |   0.9371155160628845   |   0.5555555555555556   |   0.49019607843137253   |   0.5208333333333333   |  \n",
      "Voting   |   0.9282296650717703   |   0.48672566371681414   |   0.5392156862745098   |   0.5116279069767441   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8632946001367054   |   0.26666666666666666   |   0.5490196078431373   |   0.358974358974359   |  \n",
      "LDA   |   0.8701298701298701   |   0.2755102040816326   |   0.5294117647058824   |   0.3624161073825503   |  \n",
      "KNN-5   |   0.8571428571428571   |   0.2663755458515284   |   0.5980392156862745   |   0.36858006042296076   |  \n",
      "KNN-10   |   0.898838004101162   |   0.34868421052631576   |   0.5196078431372549   |   0.4173228346456692   |  \n",
      "GNB   |   0.9200273410799726   |   0.3170731707317073   |   0.12745098039215685   |   0.1818181818181818   |  \n",
      "DT   |   0.8380041011619959   |   0.2545454545454545   |   0.6862745098039216   |   0.3713527851458886   |  \n",
      "SVC   |   0.8940533151059467   |   0.32679738562091504   |   0.49019607843137253   |   0.39215686274509803   |  \n",
      "RFC   |   0.8701298701298701   |   0.30701754385964913   |   0.6862745098039216   |   0.4242424242424242   |  \n",
      "XGB   |   0.8714969241285031   |   0.3130434782608696   |   0.7058823529411765   |   0.43373493975903615   |  \n",
      "Voting   |   0.8831168831168831   |   0.33004926108374383   |   0.6568627450980392   |   0.4393442622950819   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8448393711551606   |   0.26591760299625467   |   0.696078431372549   |   0.38482384823848237   |  \n",
      "LDA   |   0.8516746411483254   |   0.2374429223744292   |   0.5098039215686274   |   0.32398753894080995   |  \n",
      "KNN-5   |   0.7935748462064252   |   0.17532467532467533   |   0.5294117647058824   |   0.2634146341463415   |  \n",
      "KNN-10   |   0.8017771701982228   |   0.19480519480519481   |   0.5882352941176471   |   0.2926829268292683   |  \n",
      "GNB   |   0.9193438140806562   |   0.35714285714285715   |   0.19607843137254902   |   0.2531645569620253   |  \n",
      "DT   |   0.836637047163363   |   0.23754789272030652   |   0.6078431372549019   |   0.3415977961432507   |  \n",
      "SVC   |   0.8393711551606289   |   0.25461254612546125   |   0.6764705882352942   |   0.3699731903485255   |  \n",
      "RFC   |   0.9077238550922762   |   0.4   |   0.6470588235294118   |   0.49438202247191015   |  \n",
      "XGB   |   0.9104579630895421   |   0.40764331210191085   |   0.6274509803921569   |   0.49420849420849433   |  \n",
      "Voting   |   0.9056732740943267   |   0.39285714285714285   |   0.6470588235294118   |   0.4888888888888888   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------\n",
    "# EOF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
