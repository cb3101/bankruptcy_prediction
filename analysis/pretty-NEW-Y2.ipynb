{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install scikit-learn==0.20.1  # dep for missingpy\n",
    "#!pip3 install scikit-learn==0.24.1  # required for getting tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year $N$\n",
    "Run all in this section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = '2'\n",
    "drop_cols = ['Attr37', 'Attr7','Attr43','Attr62','Attr32','Attr44','Attr15','Attr19','Attr3','Attr51','Attr4','Attr49','Attr38','Attr60','Attr6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff = loadarff(data_path+N+'year.arff')\n",
    "df = pd.DataFrame(arff[0])\n",
    "df['class']= df['class'].astype('int')\n",
    "df = df.drop_duplicates()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attr37    4485\n",
       "Attr21    3159\n",
       "Attr27     703\n",
       "Attr60     539\n",
       "Attr45     537\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change dropcols accordingly...\n",
    "df.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(drop_cols,axis='columns')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain NaN in test set also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7264\n",
       "1     298\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    2421\n",
       "1     100\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop('class',axis='columns')\n",
    "Y = df['class']\n",
    "# (X.shape,Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=random_state,shuffle=True,stratify=Y)\n",
    "\n",
    "display(Y_train.value_counts(), Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test,Y_test = X_train.to_numpy(),Y_train.to_numpy(),X_test.to_numpy(),Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "#scaler = None\n",
    "\n",
    "#idx = np.isnan(X_train).any(axis=1)\n",
    "#scaler = StandardScaler().fit(X_train[~idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_imputer(imputer_estimator,X_train,Y_train,transform_x=True,reset_index = True,verbose=True,max_iter=10,tol=1,imputer=None,scaler=None):\n",
    "    # train = pd.concat([X_train,Y_train],axis=1)\n",
    "    # train['class'] = train['class'].astype('category')\n",
    "    # if reset_index:\n",
    "    #     train = train.reset_index(drop=True)\n",
    "    \n",
    "    if imputer is None:\n",
    "        imputer = IterativeImputer(estimator=imputer_estimator, n_nearest_features=None, imputation_order='descending',verbose=verbose,max_iter=max_iter,tol=tol)\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "    else:\n",
    "        imputer = imputer.fit(X_train,Y_train)\n",
    "        \n",
    "    \n",
    "    if transform_x:\n",
    "        X_train = imputer.transform(X_train)\n",
    "        \n",
    "        #if scaler is not None:\n",
    "        #    X_train = scaler.transform(X_train)\n",
    "        \n",
    "        return imputer,X_train\n",
    "    \n",
    "    return imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom missingpy import MissForest\\n\\nmissf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\\nmissf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\\nX_test_imp = missf_imp.transform(X_test)\\n\\n# note... these are not scaled!!\\nnp.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\\nnp.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from missingpy import MissForest\n",
    "\n",
    "missf_imp = MissForest(random_state=random_state,verbose=1,n_jobs=4)\n",
    "missf_imp,X_train_imp = build_imputer(None,X_train,Y_train,transform_x=True,imputer=missf_imp)\n",
    "X_test_imp = missf_imp.transform(X_test)\n",
    "\n",
    "# note... these are not scaled!!\n",
    "np.save(\"y\"+N+\"_realmissforest_train.npy\",X_train_imp)\n",
    "np.save(\"y\"+N+\"_realmissforest_test.npy\",X_test_imp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7562, 49)\n",
      "[IterativeImputer] Change: 21.240161710996357, scaled tolerance: 0.8664144971701732 \n",
      "[IterativeImputer] Change: 1.805497416595102, scaled tolerance: 0.8664144971701732 \n",
      "[IterativeImputer] Change: 0.7432344498108066, scaled tolerance: 0.8664144971701732 \n",
      "[IterativeImputer] Early stopping criterion reached.\n"
     ]
    }
   ],
   "source": [
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# has to be scaled...\n",
    "knn_imp = build_imputer( KNeighborsRegressor(n_jobs=4) ,scaler.transform(X_train),Y_train,transform_x=False,reset_index = True,verbose=True,max_iter=64,tol=0.01,scaler=scaler)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "simple_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "simple_imp = simple_imp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.01226994 0.01226994        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.0001232846739442066}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:02:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 6.309573444801943, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.1297431 0.1297431       nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.519911082952933e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 9}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:13:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.6,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.12192007 0.12296324        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'lsqr'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.232846739442066e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 8}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:16:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 5,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10000.0, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.13814529 0.13814529        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.8111308307896871}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 15}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:29:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "Simple & No & Acc & 0.96 & 0.96 & 0.96 & 0.96 & 0.06 & 0.97 & 0.96 & 0.97 & 0.97 & 0.97 \\\\\n",
      "~ & ~ & Prec & 0.33 & 0.40 & 0.00 & 0.00 & 0.04 & 0.71 & 0.00 & 0.85 & 0.93 & 0.95 \\\\\n",
      "~ & ~ & Rec & 0.02 & 0.02 & 0.00 & 0.00 & 0.93 & 0.25 & 0.00 & 0.22 & 0.37 & 0.20 \\\\\n",
      "~ & ~ & F1 & 0.04 & 0.04 & 0.00 & 0.00 & 0.07 & 0.37 & 0.00 & 0.35 & 0.53 & 0.33 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.86 & 0.93 & 0.81 & 0.80 & 0.08 & 0.92 & 0.86 & 0.97 & 0.97 & 0.97 \\\\\n",
      "~ & ~ & Prec & 0.12 & 0.07 & 0.08 & 0.07 & 0.04 & 0.20 & 0.13 & 0.74 & 0.67 & 0.67 \\\\\n",
      "~ & ~ & Rec & 0.40 & 0.06 & 0.36 & 0.34 & 0.90 & 0.36 & 0.41 & 0.28 & 0.39 & 0.34 \\\\\n",
      "~ & ~ & F1 & 0.19 & 0.06 & 0.13 & 0.12 & 0.07 & 0.26 & 0.19 & 0.41 & 0.49 & 0.45 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.82 & 0.84 & 0.73 & 0.81 & 0.88 & 0.86 & 0.87 & 0.87 & 0.88 & 0.91 \\\\\n",
      "~ & ~ & Prec & 0.10 & 0.08 & 0.08 & 0.08 & 0.06 & 0.16 & 0.07 & 0.18 & 0.21 & 0.24 \\\\\n",
      "~ & ~ & Rec & 0.46 & 0.28 & 0.53 & 0.36 & 0.14 & 0.60 & 0.20 & 0.66 & 0.69 & 0.62 \\\\\n",
      "~ & ~ & F1 & 0.17 & 0.12 & 0.13 & 0.13 & 0.09 & 0.25 & 0.11 & 0.28 & 0.32 & 0.35 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.72 & 0.83 & 0.74 & 0.73 & 0.05 & 0.88 & 0.78 & 0.96 & 0.96 & 0.95 \\\\\n",
      "~ & ENN & Prec & 0.08 & 0.07 & 0.07 & 0.07 & 0.04 & 0.15 & 0.09 & 0.52 & 0.50 & 0.42 \\\\\n",
      "~ & ~ & Rec & 0.55 & 0.28 & 0.43 & 0.48 & 0.99 & 0.41 & 0.51 & 0.26 & 0.39 & 0.45 \\\\\n",
      "~ & ~ & F1 & 0.13 & 0.11 & 0.12 & 0.12 & 0.08 & 0.21 & 0.15 & 0.35 & 0.44 & 0.43 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"Simple & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9595398651328838   |   0.3333333333333333   |   0.02   |   0.03773584905660377   |  \n",
      "LDA   |   0.9599365331217771   |   0.4   |   0.02   |   0.03809523809523809   |  \n",
      "KNN-5   |   0.9583498611662039   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.0646568821896073   |   0.03805237315875614   |   0.93   |   0.07311320754716982   |  \n",
      "DT   |   0.9662832209440698   |   0.7142857142857143   |   0.25   |   0.37037037037037035   |  \n",
      "SVC   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9674732249107497   |   0.8461538461538461   |   0.22   |   0.3492063492063492   |  \n",
      "XGB   |   0.9738199127330425   |   0.925   |   0.37   |   0.5285714285714286   |  \n",
      "Voting   |   0.967869892899643   |   0.9523809523809523   |   0.2   |   0.3305785123966942   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8603728679095597   |   0.12048192771084337   |   0.4   |   0.18518518518518517   |  \n",
      "LDA   |   0.9313764379214597   |   0.07058823529411765   |   0.06   |   0.06486486486486485   |  \n",
      "KNN-5   |   0.8131693772312575   |   0.08126410835214447   |   0.36   |   0.13259668508287292   |  \n",
      "KNN-10   |   0.7961126537088457   |   0.07053941908713693   |   0.34   |   0.11683848797250859   |  \n",
      "GNB   |   0.08409361364537882   |   0.037672666387609875   |   0.9   |   0.07231820008035354   |  \n",
      "DT   |   0.9166997223324078   |   0.1978021978021978   |   0.36   |   0.2553191489361702   |  \n",
      "SVC   |   0.8647362157873859   |   0.12693498452012383   |   0.41   |   0.1938534278959811   |  \n",
      "RFC   |   0.9674732249107497   |   0.7368421052631579   |   0.28   |   0.40579710144927544   |  \n",
      "XGB   |   0.9682665608885362   |   0.6724137931034483   |   0.39   |   0.4936708860759494   |  \n",
      "Voting   |   0.9670765569218565   |   0.6666666666666666   |   0.34   |   0.45033112582781465   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8187227290757636   |   0.10244988864142539   |   0.46   |   0.16757741347905283   |  \n",
      "LDA   |   0.8441094803649345   |   0.08022922636103152   |   0.28   |   0.12472160356347439   |  \n",
      "KNN-5   |   0.7262990876636255   |   0.07614942528735633   |   0.53   |   0.13316582914572864   |  \n",
      "KNN-10   |   0.8092026973423245   |   0.07947019867549669   |   0.36   |   0.1301989150090416   |  \n",
      "GNB   |   0.8849662832209441   |   0.06422018348623854   |   0.14   |   0.08805031446540881   |  \n",
      "DT   |   0.8583895279650933   |   0.15915119363395225   |   0.6   |   0.25157232704402516   |  \n",
      "SVC   |   0.8675128917096391   |   0.072992700729927   |   0.2   |   0.10695187165775402   |  \n",
      "RFC   |   0.8671162237207457   |   0.17983651226158037   |   0.66   |   0.2826552462526767   |  \n",
      "XGB   |   0.8837762792542642   |   0.2084592145015106   |   0.69   |   0.32018561484918795   |  \n",
      "Voting   |   0.9079730265767553   |   0.2421875   |   0.62   |   0.348314606741573   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.7167790559301864   |   0.07596685082872928   |   0.55   |   0.13349514563106799   |  \n",
      "LDA   |   0.8250694168980564   |   0.07052896725440806   |   0.28   |   0.11267605633802817   |  \n",
      "KNN-5   |   0.738992463308211   |   0.06677018633540373   |   0.43   |   0.11559139784946237   |  \n",
      "KNN-10   |   0.7294724315747719   |   0.07079646017699115   |   0.48   |   0.12339331619537273   |  \n",
      "GNB   |   0.04601348671162237   |   0.039552536955653216   |   0.99   |   0.07606607760276603   |  \n",
      "DT   |   0.8809996033320111   |   0.1453900709219858   |   0.41   |   0.21465968586387432   |  \n",
      "SVC   |   0.7786592621975407   |   0.09107142857142857   |   0.51   |   0.15454545454545454   |  \n",
      "RFC   |   0.961126537088457   |   0.52   |   0.26   |   0.3466666666666667   |  \n",
      "XGB   |   0.9603332011106703   |   0.5   |   0.39   |   0.43820224719101125   |  \n",
      "Voting   |   0.9535898452994843   |   0.4205607476635514   |   0.45   |   0.43478260869565216   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MissForest Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.01234568 0.01234568        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 8.111308307896872e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 9}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:46:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.11551416 0.11802633        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'lsqr'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 2.310129700083158e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[21:59:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 251.18864315095823, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.14650707 0.1462037         nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.873817422860383e-07}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:00:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 2,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.13407308 0.13400076        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 4.3287612810830526e-08}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "\t- Requires scaling and not scaled. Doing it now...\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:13:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "MissForest & No & Acc & 0.96 & 0.96 & 0.96 & 0.96 & 0.08 & 0.95 & 0.96 & 0.96 & 0.97 & 0.96 \\\\\n",
      "~ & ~ & Prec & 0.33 & 0.25 & 0.00 & 0.00 & 0.04 & 0.37 & 0.00 & 0.12 & 0.80 & 1.00 \\\\\n",
      "~ & ~ & Rec & 0.02 & 0.04 & 0.00 & 0.00 & 0.89 & 0.21 & 0.00 & 0.01 & 0.16 & 0.01 \\\\\n",
      "~ & ~ & F1 & 0.04 & 0.07 & 0.00 & 0.00 & 0.07 & 0.27 & 0.00 & 0.02 & 0.27 & 0.02 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.83 & 0.93 & 0.80 & 0.79 & 0.09 & 0.83 & 0.83 & 0.92 & 0.96 & 0.94 \\\\\n",
      "~ & ~ & Prec & 0.10 & 0.09 & 0.08 & 0.07 & 0.04 & 0.11 & 0.10 & 0.20 & 0.45 & 0.25 \\\\\n",
      "~ & ~ & Rec & 0.40 & 0.08 & 0.38 & 0.35 & 0.89 & 0.46 & 0.43 & 0.31 & 0.34 & 0.28 \\\\\n",
      "~ & ~ & F1 & 0.16 & 0.09 & 0.13 & 0.11 & 0.07 & 0.18 & 0.16 & 0.24 & 0.39 & 0.26 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.82 & 0.86 & 0.72 & 0.80 & 0.92 & 0.77 & 0.86 & 0.83 & 0.84 & 0.87 \\\\\n",
      "~ & ~ & Prec & 0.08 & 0.08 & 0.07 & 0.06 & 0.06 & 0.09 & 0.06 & 0.14 & 0.15 & 0.14 \\\\\n",
      "~ & ~ & Rec & 0.33 & 0.24 & 0.50 & 0.27 & 0.07 & 0.52 & 0.18 & 0.62 & 0.64 & 0.45 \\\\\n",
      "~ & ~ & F1 & 0.13 & 0.12 & 0.12 & 0.10 & 0.07 & 0.15 & 0.09 & 0.22 & 0.24 & 0.22 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.71 & 0.83 & 0.73 & 0.72 & 0.09 & 0.82 & 0.76 & 0.88 & 0.93 & 0.88 \\\\\n",
      "~ & ENN & Prec & 0.07 & 0.07 & 0.07 & 0.07 & 0.04 & 0.09 & 0.08 & 0.14 & 0.27 & 0.15 \\\\\n",
      "~ & ~ & Rec & 0.55 & 0.24 & 0.47 & 0.46 & 0.89 & 0.40 & 0.50 & 0.37 & 0.37 & 0.45 \\\\\n",
      "~ & ~ & F1 & 0.13 & 0.10 & 0.12 & 0.12 & 0.07 & 0.15 & 0.14 & 0.20 & 0.31 & 0.23 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"MissForest & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9595398651328838   |   0.3333333333333333   |   0.02   |   0.03773584905660377   |  \n",
      "LDA   |   0.957159857199524   |   0.25   |   0.04   |   0.06896551724137932   |  \n",
      "KNN-5   |   0.9579531931773105   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.07616025386751289   |   0.03697548815953469   |   0.89   |   0.07100119664938173   |  \n",
      "DT   |   0.954383181277271   |   0.3684210526315789   |   0.21   |   0.267515923566879   |  \n",
      "SVC   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9579531931773105   |   0.125   |   0.01   |   0.018518518518518517   |  \n",
      "XGB   |   0.9650932169773899   |   0.8   |   0.16   |   0.26666666666666666   |  \n",
      "Voting   |   0.9607298690995637   |   1.0   |   0.01   |   0.019801980198019802   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8322094406981356   |   0.09925558312655088   |   0.4   |   0.1590457256461233   |  \n",
      "LDA   |   0.9321697738992464   |   0.09195402298850575   |   0.08   |   0.0855614973262032   |  \n",
      "KNN-5   |   0.7984926616422054   |   0.07851239669421488   |   0.38   |   0.13013698630136986   |  \n",
      "KNN-10   |   0.78579928599762   |   0.06862745098039216   |   0.35   |   0.11475409836065575   |  \n",
      "GNB   |   0.09044030146767155   |   0.03753690425980599   |   0.89   |   0.07203561311210037   |  \n",
      "DT   |   0.8349861166203887   |   0.11274509803921569   |   0.46   |   0.18110236220472442   |  \n",
      "SVC   |   0.8270527568425228   |   0.1018957345971564   |   0.43   |   0.1647509578544061   |  \n",
      "RFC   |   0.9222530741769139   |   0.1962025316455696   |   0.31   |   0.24031007751937986   |  \n",
      "XGB   |   0.957159857199524   |   0.4473684210526316   |   0.34   |   0.38636363636363635   |  \n",
      "Voting   |   0.9381197937326458   |   0.25   |   0.28   |   0.2641509433962264   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8218960729869099   |   0.07951807228915662   |   0.33   |   0.12815533980582525   |  \n",
      "LDA   |   0.859579531931773   |   0.07947019867549669   |   0.24   |   0.11940298507462686   |  \n",
      "KNN-5   |   0.7151923839746133   |   0.06963788300835655   |   0.5   |   0.12224938875305623   |  \n",
      "KNN-10   |   0.7969059896866323   |   0.05793991416309013   |   0.27   |   0.09540636042402827   |  \n",
      "GNB   |   0.9214597381991273   |   0.0625   |   0.07   |   0.0660377358490566   |  \n",
      "DT   |   0.7723125743752479   |   0.08996539792387544   |   0.52   |   0.15339233038348082   |  \n",
      "SVC   |   0.859579531931773   |   0.06206896551724138   |   0.18   |   0.0923076923076923   |  \n",
      "RFC   |   0.8282427608092027   |   0.13566739606126915   |   0.62   |   0.22262118491921004   |  \n",
      "XGB   |   0.839349464498215   |   0.14780600461893764   |   0.64   |   0.2401500938086304   |  \n",
      "Voting   |   0.8702895676318921   |   0.14195583596214512   |   0.45   |   0.21582733812949642   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.7080523601745339   |   0.07372654155495978   |   0.55   |   0.13002364066193853   |  \n",
      "LDA   |   0.8349861166203887   |   0.06593406593406594   |   0.24   |   0.10344827586206898   |  \n",
      "KNN-5   |   0.7330424434748116   |   0.0704647676161919   |   0.47   |   0.12255541069100391   |  \n",
      "KNN-10   |   0.721539071796906   |   0.06628242074927954   |   0.46   |   0.1158690176322418   |  \n",
      "GNB   |   0.09202697342324474   |   0.03760033798056612   |   0.89   |   0.07215241183623834   |  \n",
      "DT   |   0.8151527171757239   |   0.08968609865470852   |   0.4   |   0.14652014652014653   |  \n",
      "SVC   |   0.7560491868306227   |   0.08130081300813008   |   0.5   |   0.13986013986013987   |  \n",
      "RFC   |   0.8833796112653709   |   0.13805970149253732   |   0.37   |   0.20108695652173914   |  \n",
      "XGB   |   0.9349464498214994   |   0.26811594202898553   |   0.37   |   0.3109243697478992   |  \n",
      "Voting   |   0.8770329234430781   |   0.15   |   0.45   |   0.22500000000000003   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7562, 49)\n",
      "[IterativeImputer] Completing matrix with shape (2521, 49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#### for missf, use saved files...\\nscaled_already = False\\nX_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\\nX_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose imputer <<comment blocks accordingly>>\n",
    "'''\n",
    "#### for simple\n",
    "scaled_already = False\n",
    "X_train_imp = simple_imp.transform(X_train)\n",
    "X_test_imp = simple_imp.transform(X_test)\n",
    "'''\n",
    "################################# OR ################################\n",
    "\n",
    "#### for KNN\n",
    "scaled_already = True\n",
    "X_train_imp = knn_imp.transform(scaler.transform(X_train))\n",
    "X_test_imp = knn_imp.transform(scaler.transform(X_test))\n",
    "\n",
    "################################# OR ################################\n",
    "'''\n",
    "#### for missf, use saved files...\n",
    "scaled_already = False\n",
    "X_train_imp = np.load(\"y\"+N+\"_realmissforest_train.npy\")\n",
    "X_test_imp = np.load(\"y\"+N+\"_realmissforest_test.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier # Voting Ensemble for Classification\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_classifiers(X_train, X_test, y_train, y_test, classifiers, sampling  = None, scaler=None):\n",
    "    '''\n",
    "    do all imputations before passing here...\n",
    "    Classifier : array of tuples (classifier,scaling required=True/False)\n",
    "    '''\n",
    "    accuracy = [0]*len(classifiers)\n",
    "    f1 = [0]*len(classifiers)\n",
    "    precision = [0]*len(classifiers)\n",
    "    recall = [0]*len(classifiers)\n",
    "    i = 0\n",
    "    \n",
    "    model_pipeline = []\n",
    "    Pipeline([\n",
    "        ('sampling', SMOTE()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    if sampling == \"SMOTE\":\n",
    "        model_pipeline.append(('sampling', SMOTE(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTE\")\n",
    "    if sampling == \"RUS\":\n",
    "        model_pipeline.append(('sampling', RandomUnderSampler(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "        print(\"RUS\")\n",
    "    if sampling == \"SMOTEENN\":\n",
    "        model_pipeline.append(('sampling', SMOTEENN(sampling_strategy=0.6,random_state=random_state) ))\n",
    "        # X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n",
    "        print(\"SMOTEENN\")    \n",
    "\n",
    "    voting_classifs = []\n",
    "    models_for_voting = [0,3,5,6,7,8]\n",
    "    voting_weights = [1,0.5,1,0.5,1,1]\n",
    "    jj =0 \n",
    "        \n",
    "    for i in range(len(classifiers)):\n",
    "        classif = classifiers[i][0]\n",
    "        pipe_parameters = classifiers[i][2]\n",
    "        y_pred = []\n",
    "        print(classifiers_names[i])\n",
    "        pipeline = Pipeline(model_pipeline+[('classifier',classif)])\n",
    "        \n",
    "        if classifiers[i][1] and not scaled_already:\n",
    "            print(\"\\t- Requires scaling and not scaled. Doing it now...\")\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            #grid = grid.fit(X_train, y_train)\n",
    "            grid = grid.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "            #classif.fit(scaler.transform(X_train), y_train)\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(scaler.transform(X_test))\n",
    "            #classif.fit(scaler.transform(), )\n",
    "            #y_pred = classif.predict(scaler.transform(X_test))\n",
    "            \n",
    "        else:\n",
    "            grid = GridSearchCV(pipeline, pipe_parameters, cv=2, scoring=\"f1\",n_jobs=-1,verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            display(grid.best_params_)\n",
    "            classif = grid.best_estimator_\n",
    "            y_pred = grid.predict(X_test)\n",
    "            \n",
    "            #classif.fit(X_train, y_train)\n",
    "            #y_pred = classif.predict(X_test)\n",
    "        \n",
    "        if i in models_for_voting:\n",
    "            print(\"\\t- Adding for voting with weight <\"+str(voting_weights[jj])+\">...\")\n",
    "            jj+=1\n",
    "            voting_classifs.append((\"mod\"+str(i+1),classif))\n",
    "        \n",
    "        accuracy[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "        f1[i] = metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "        precision[i] = metrics.precision_score(y_test, y_pred)\n",
    "        recall[i] = metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\\nVoting...\")\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(voting_classifs,weights=voting_weights,n_jobs=-1,voting=\"hard\")\n",
    "    ensemble.fit(scaler.transform(X_train), y_train)\n",
    "    y_pred = ensemble.predict(scaler.transform(X_test))\n",
    "    \n",
    "    accuracy.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1.append(metrics.f1_score(y_test, y_pred, labels=np.unique(y_pred)))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Done\")\n",
    "    return accuracy,f1,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers_voting = [('log',LogisticRegression(max_iter=2048)),(\"knn10\",KNeighborsClassifier(n_neighbors=10)),(\"dtc\",DecisionTreeClassifier()),(\"svm_linear\",SVC(kernel='linear',random_state=random_state)),(\"rf\",RandomForestClassifier(n_estimators=16, n_jobs=8, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "# classifiers_voting = [(\"dtc\",DecisionTreeClassifier()),(\"rf\",RandomForestClassifier(n_estimators=64, n_jobs=-1, random_state=random_state)),(\"xbg\",XGBClassifier(use_label_encoder=False))]\n",
    "\n",
    "classifiers_names = [\"LR\", \"LDA\", \"KNN-5\", \"KNN-10\", \"GNB\", \"DT\", \"SVC\", \"RFC\", \"XGB\",\"Voting\"]\n",
    "\n",
    "classifiers = [(LogisticRegression(max_iter=2048,random_state=random_state),True,\n",
    "                   [\n",
    "                       #{\n",
    "                       # 'classifier__penalty' : ['l1', 'l2'],\n",
    "                       # 'classifier__C' : np.logspace(-8, 4, 16),\n",
    "                       # 'classifier__solver' : ['liblinear']\n",
    "                       # },\n",
    "                        {\n",
    "                        'classifier__penalty' : ['l2','none'],\n",
    "                        'classifier__C' : np.logspace(-8, 4, 16)\n",
    "                        }\n",
    "                   ]),\n",
    "                (LinearDiscriminantAnalysis(),True,\n",
    "                    { 'classifier__solver' : ['svd', 'lsqr', 'eigen'] }),\n",
    "                (KNeighborsClassifier(n_neighbors=5),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}), \n",
    "                (KNeighborsClassifier(n_neighbors=10),True,\n",
    "                    {'classifier__weights' : ['uniform','distance'], 'classifier__metric' : ['euclidean', 'manhattan']}),\n",
    "                (GaussianNB(),True,\n",
    "                    {'classifier__var_smoothing': np.logspace(0,-9, num=100)}),\n",
    "                (DecisionTreeClassifier(random_state=random_state),False,\n",
    "                    { 'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]} ),\n",
    "                (SVC(random_state=random_state),True,\n",
    "                    [\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1], \n",
    "                         'classifier__gamma': [0.0001, 1],\n",
    "                         'classifier__kernel': ['rbf']},\n",
    "                        {'classifier__C': [ 0.05, 0.1, 1],\n",
    "                         'classifier__kernel': ['linear']}\n",
    "                    ]),\n",
    "                (RandomForestClassifier(random_state=random_state),False,\n",
    "                     { 'classifier__n_estimators': [int(x) for x in np.linspace(start = 128, stop = 512, num = 4)],\n",
    "                       'classifier__max_features': ['auto'],\n",
    "                       'classifier__max_depth':  [int(x) for x in np.linspace(10, 100, num = 2)]+[None],\n",
    "                       'classifier__min_samples_leaf':  [1, 4],\n",
    "                       'classifier__bootstrap': [False]\n",
    "                     }),\n",
    "                (XGBClassifier(use_label_encoder=False),False,\n",
    "                    {\n",
    "                        'classifier__gamma': [0.5, 1, 2, 5],\n",
    "                        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                        'classifier__max_depth': [3, 6]\n",
    "                    }) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1e-08, 'classifier__penalty': 'none'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.01212121 0.01212121        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'euclidean', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 4.328761281083053e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'entropy', 'classifier__max_depth': 20}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05,\n",
       " 'classifier__gamma': 0.0001,\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 100,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 384}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:24:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1,precision,recall = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 6.309573444801943, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.13353582 0.13353582        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 1.873817422860383e-06}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:35:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_sm,f1_sm,precision_sm,recall_sm = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTE\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1584.8931924611175, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.12953066 0.13183115        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'lsqr'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 6.579332246575682e-09}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 4}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__n_estimators': 512}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:37:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 1,\n",
       " 'classifier__max_depth': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_rus,f1_rus,precision_rus,recall_rus = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"RUS\", scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN\n",
      "LR\n",
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 6.309573444801943, 'classifier__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "LDA\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.14611198 0.14604347        nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__solver': 'svd'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-5\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN-10\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'manhattan', 'classifier__weights': 'uniform'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "GNB\n",
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__var_smoothing': 0.8111308307896871}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__criterion': 'gini', 'classifier__max_depth': 5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "SVC\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.05, 'classifier__kernel': 'linear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <0.5>...\n",
      "RFC\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': False,\n",
       " 'classifier__max_depth': 10,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__min_samples_leaf': 4,\n",
       " 'classifier__n_estimators': 256}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "XGB\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[22:52:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__colsample_bytree': 1.0,\n",
       " 'classifier__gamma': 0.5,\n",
       " 'classifier__max_depth': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Adding for voting with weight <1>...\n",
      "\n",
      "\n",
      "Voting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_smoteenn,f1_smoteenn,precision_smoteenn,recall_smoteenn = try_all_classifiers(X_train_imp,X_test_imp,Y_train,Y_test, classifiers, sampling = \"SMOTEENN\",scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer & Sampling & Metric & LR & LDA & KNN-5 & KNN-10 & GNB & DT & SVC & RFC & XGB & Voting \\\\\n",
      "\\hline \\hline\n",
      "KNN & No & Acc & 0.96 & 0.96 & 0.96 & 0.96 & 0.07 & 0.94 & 0.96 & 0.96 & 0.96 & 0.96 \\\\\n",
      "~ & ~ & Prec & 0.33 & 0.40 & 0.00 & 0.00 & 0.04 & 0.19 & 0.00 & 0.12 & 0.73 & 0.33 \\\\\n",
      "~ & ~ & Rec & 0.02 & 0.02 & 0.00 & 0.00 & 0.91 & 0.18 & 0.00 & 0.01 & 0.16 & 0.01 \\\\\n",
      "~ & ~ & F1 & 0.04 & 0.04 & 0.00 & 0.00 & 0.07 & 0.18 & 0.00 & 0.02 & 0.26 & 0.02 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE & Acc & 0.85 & 0.93 & 0.80 & 0.78 & 0.08 & 0.86 & 0.83 & 0.93 & 0.92 & 0.93 \\\\\n",
      "~ & ~ & Prec & 0.12 & 0.07 & 0.08 & 0.06 & 0.04 & 0.12 & 0.10 & 0.19 & 0.21 & 0.23 \\\\\n",
      "~ & ~ & Rec & 0.42 & 0.06 & 0.36 & 0.32 & 0.91 & 0.39 & 0.40 & 0.27 & 0.35 & 0.31 \\\\\n",
      "~ & ~ & F1 & 0.18 & 0.06 & 0.12 & 0.10 & 0.07 & 0.19 & 0.16 & 0.22 & 0.27 & 0.26 \\\\\n",
      "\\cline{2-13}\n",
      "~ & RUS & Acc & 0.81 & 0.85 & 0.71 & 0.79 & 0.89 & 0.82 & 0.86 & 0.82 & 0.82 & 0.88 \\\\\n",
      "~ & ~ & Prec & 0.10 & 0.09 & 0.07 & 0.06 & 0.06 & 0.11 & 0.07 & 0.12 & 0.12 & 0.15 \\\\\n",
      "~ & ~ & Rec & 0.44 & 0.30 & 0.52 & 0.31 & 0.13 & 0.49 & 0.19 & 0.59 & 0.58 & 0.45 \\\\\n",
      "~ & ~ & F1 & 0.16 & 0.14 & 0.12 & 0.11 & 0.08 & 0.18 & 0.10 & 0.20 & 0.20 & 0.23 \\\\\n",
      "\\cline{2-13}\n",
      "~ & SMOTE- & Acc & 0.70 & 0.82 & 0.73 & 0.72 & 0.05 & 0.67 & 0.75 & 0.86 & 0.93 & 0.88 \\\\\n",
      "~ & ENN & Prec & 0.08 & 0.07 & 0.06 & 0.07 & 0.04 & 0.07 & 0.08 & 0.12 & 0.24 & 0.14 \\\\\n",
      "~ & ~ & Rec & 0.59 & 0.28 & 0.43 & 0.46 & 0.99 & 0.56 & 0.51 & 0.42 & 0.37 & 0.40 \\\\\n",
      "~ & ~ & F1 & 0.14 & 0.11 & 0.11 & 0.11 & 0.08 & 0.12 & 0.14 & 0.19 & 0.29 & 0.20 \\\\\n",
      "\\hline\\hline\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputer & Sampling & Metric & \",end = \"\")\n",
    "print(*classifiers_names,sep = \" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline \\\\hline\")\n",
    "print(\"KNN & No & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & SMOTE & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_sm],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "print(\"~ & RUS & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_rus],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\cline{2-13}\")\n",
    "\n",
    "\n",
    "print(\"~ & SMOTE- & Acc & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in accuracy_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ENN & Prec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in precision_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & Rec & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in recall_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"~ & ~ & F1 & \",end=\"\")\n",
    "print(*['%.2f' % elem for elem in f1_smoteenn],sep=\" & \", end = \" \\\\\\\\\\n\")\n",
    "print(\"\\\\hline\\\\hline\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
      "| =================== | ======== | ========= | ====== | ======== |\n",
      "LR   |   0.9595398651328838   |   0.3333333333333333   |   0.02   |   0.03773584905660377   |  \n",
      "LDA   |   0.9599365331217771   |   0.4   |   0.02   |   0.03809523809523809   |  \n",
      "KNN-5   |   0.9583498611662039   |   0.0   |   0.0   |   0.0   |  \n",
      "KNN-10   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "GNB   |   0.06624355414518049   |   0.03735632183908046   |   0.91   |   0.0717665615141956   |  \n",
      "DT   |   0.9361364537881793   |   0.18556701030927836   |   0.18   |   0.18274111675126903   |  \n",
      "SVC   |   0.9603332011106703   |   0.0   |   0.0   |   0.0   |  \n",
      "RFC   |   0.9579531931773105   |   0.125   |   0.01   |   0.018518518518518517   |  \n",
      "XGB   |   0.9642998809996033   |   0.7272727272727273   |   0.16   |   0.2622950819672131   |  \n",
      "Voting   |   0.9599365331217771   |   0.3333333333333333   |   0.01   |   0.019417475728155338   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8516461721539071   |   0.11731843575418995   |   0.42   |   0.18340611353711792   |  \n",
      "LDA   |   0.9305831019436731   |   0.06896551724137931   |   0.06   |   0.06417112299465241   |  \n",
      "KNN-5   |   0.7992859976199921   |   0.07531380753138076   |   0.36   |   0.12456747404844291   |  \n",
      "KNN-10   |   0.7810392701309005   |   0.06201550387596899   |   0.32   |   0.1038961038961039   |  \n",
      "GNB   |   0.07774692582308608   |   0.037806398005816366   |   0.91   |   0.07259672915835659   |  \n",
      "DT   |   0.8647362157873859   |   0.12225705329153605   |   0.39   |   0.18615751789976137   |  \n",
      "SVC   |   0.8294327647758826   |   0.0975609756097561   |   0.4   |   0.15686274509803924   |  \n",
      "RFC   |   0.9254264180880603   |   0.19014084507042253   |   0.27   |   0.22314049586776863   |  \n",
      "XGB   |   0.9230464101547006   |   0.21341463414634146   |   0.35   |   0.2651515151515152   |  \n",
      "Voting   |   0.9313764379214597   |   0.22962962962962963   |   0.31   |   0.26382978723404255   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.8131693772312575   |   0.09586056644880174   |   0.44   |   0.15742397137745975   |  \n",
      "LDA   |   0.850059500198334   |   0.08875739644970414   |   0.3   |   0.136986301369863   |  \n",
      "KNN-5   |   0.7100357001190004   |   0.0707482993197279   |   0.52   |   0.12455089820359282   |  \n",
      "KNN-10   |   0.7949226497421658   |   0.06471816283924843   |   0.31   |   0.10708117443868738   |  \n",
      "GNB   |   0.886156287187624   |   0.06103286384976526   |   0.13   |   0.08306709265175719   |  \n",
      "DT   |   0.817929393097977   |   0.10722100656455143   |   0.49   |   0.17594254937163376   |  \n",
      "SVC   |   0.859579531931773   |   0.06506849315068493   |   0.19   |   0.09693877551020409   |  \n",
      "RFC   |   0.8175327251090837   |   0.12343096234309624   |   0.59   |   0.2041522491349481   |  \n",
      "XGB   |   0.8151527171757239   |   0.12033195020746888   |   0.58   |   0.1993127147766323   |  \n",
      "Voting   |   0.8786195953986513   |   0.15202702702702703   |   0.45   |   0.2272727272727273   |  \n",
      "\n",
      "===============================================================\n",
      "\n",
      "LR   |   0.7044823482744943   |   0.07732634338138926   |   0.59   |   0.13673232908458866   |  \n",
      "LDA   |   0.8226894089646966   |   0.06947890818858561   |   0.28   |   0.1113320079522863   |  \n",
      "KNN-5   |   0.7290757635858787   |   0.06427503736920777   |   0.43   |   0.11183355006501951   |  \n",
      "KNN-10   |   0.7183657278857596   |   0.06552706552706553   |   0.46   |   0.11471321695760599   |  \n",
      "GNB   |   0.04601348671162237   |   0.039552536955653216   |   0.99   |   0.07606607760276603   |  \n",
      "DT   |   0.6727489091630305   |   0.06690561529271206   |   0.56   |   0.11953041622198504   |  \n",
      "SVC   |   0.7532725109083697   |   0.08173076923076923   |   0.51   |   0.1408839779005525   |  \n",
      "RFC   |   0.8583895279650933   |   0.12316715542521994   |   0.42   |   0.19047619047619047   |  \n",
      "XGB   |   0.9293930979769932   |   0.24342105263157895   |   0.37   |   0.29365079365079366   |  \n",
      "Voting   |   0.875446251487505   |   0.1360544217687075   |   0.4   |   0.20304568527918782   |  \n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "| classifier          | Accuracy | Precision | Recall | F1 score |\n",
    "| =================== | ======== | ========= | ====== | ======== |''')\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy,precision,recall,f1):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "    \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_sm,precision_sm,recall_sm,f1_sm):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "\n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_rus,precision_rus,recall_rus,f1_rus):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    "  \n",
    "print(\"\\n===============================================================\\n\")\n",
    "for c,a,p,r,f in zip(classifiers_names,accuracy_smoteenn,precision_smoteenn,recall_smoteenn,f1_smoteenn):\n",
    "    print(c,\"  |  \",a,\"  |  \",p,\"  |  \",r,\"  |  \",f,\"  |  \")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------\n",
    "# EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
